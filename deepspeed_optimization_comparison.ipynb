{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üöÄ Comparaci√≥n de Rendimiento: Entrenamiento con y sin DeepSpeed\n",
    "\n",
    "Este notebook demuestra la diferencia en tiempo y consumo de GPU entre entrenar un modelo con y sin optimizaciones de DeepSpeed.\n",
    "\n",
    "## üìã Objetivos:\n",
    "1. Instalar las librer√≠as necesarias\n",
    "2. Crear un dataset sint√©tico para entrenamiento\n",
    "3. Configurar un modelo base para fine-tuning\n",
    "4. Entrenar SIN DeepSpeed y medir rendimiento\n",
    "5. Entrenar CON DeepSpeed y medir rendimiento\n",
    "6. Comparar resultados y analizar beneficios\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82ac9e19",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üîß Paso 1: Instalaci√≥n de Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce37f353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\impor\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ransformers (C:\\Users\\impor\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\impor\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ransformers (C:\\Users\\impor\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\impor\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# INSTALACI√ìN DE LIBRER√çAS PARA COMPARACI√ìN DEEPSPEED\n",
    "# ============================================================\n",
    "\n",
    "# üî• LIBRER√çAS ESENCIALES (obligatorias)\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install transformers  # Para modelos y tokenizers\n",
    "%pip install datasets      # Para manejo de datasets\n",
    "%pip install deepspeed    # ‚≠ê La estrella del show!\n",
    "%pip install huggingface_hub   # Para manejo de datasets y modelos\n",
    "\n",
    "# üõ†Ô∏è LIBRER√çAS DE SOPORTE (recomendadas)\n",
    "%pip install accelerate    # Facilita integraci√≥n con DeepSpeed\n",
    "%pip install trl           # Para Supervised Fine-Tuning (SFT)\n",
    "\n",
    "# üìä MONITOREO Y VISUALIZACI√ìN\n",
    "%pip install psutil        # Monitor de CPU/RAM\n",
    "%pip install GPUtil        # Monitor de GPU\n",
    "%pip install matplotlib    # Gr√°ficos\n",
    "%pip install seaborn      # Gr√°ficos bonitos\n",
    "%pip install pandas       # Tablas de datos\n",
    "%pip install numpy        # Operaciones num√©ricas\n",
    "\n",
    "# üî¨ OPTIMIZACIONES AVANZADAS (opcionales para este ejemplo)\n",
    "# %pip install peft           # Para LoRA/QLoRA (descomenta si quieres experimentar)\n",
    "# %pip install bitsandbytes   # Para cuantizaci√≥n 8-bit/4-bit (descomenta si quieres experimentar)\n",
    "# %pip install wandb          # Para logging avanzado (descomenta si tienes cuenta)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üì¶ Paso 2: Importaci√≥n de M√≥dulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5c5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "import deepspeed\n",
    "import time\n",
    "import psutil\n",
    "import GPUtil\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar para mostrar gr√°ficos\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üõ†Ô∏è Paso 3: Funciones de Utilidad para Monitoreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4270cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceMonitor:\n",
    "    \"\"\"Clase para monitorear rendimiento durante el entrenamiento\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'gpu_memory_used': [],\n",
    "            'gpu_utilization': [],\n",
    "            'cpu_percent': [],\n",
    "            'ram_used': [],\n",
    "            'timestamps': []\n",
    "        }\n",
    "        \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"Inicia el monitoreo de recursos\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def log_metrics(self):\n",
    "        \"\"\"Registra m√©tricas actuales del sistema\"\"\"\n",
    "        # GPU metrics\n",
    "        if torch.cuda.is_available():\n",
    "            gpu = GPUtil.getGPUs()[0] if GPUtil.getGPUs() else None\n",
    "            if gpu:\n",
    "                self.metrics['gpu_memory_used'].append(gpu.memoryUsed)\n",
    "                self.metrics['gpu_utilization'].append(gpu.load * 100)\n",
    "            else:\n",
    "                self.metrics['gpu_memory_used'].append(0)\n",
    "                self.metrics['gpu_utilization'].append(0)\n",
    "        else:\n",
    "            self.metrics['gpu_memory_used'].append(0)\n",
    "            self.metrics['gpu_utilization'].append(0)\n",
    "            \n",
    "        # CPU y RAM\n",
    "        self.metrics['cpu_percent'].append(psutil.cpu_percent())\n",
    "        self.metrics['ram_used'].append(psutil.virtual_memory().used / (1024**3))  # GB\n",
    "        self.metrics['timestamps'].append(time.time() - self.start_time)\n",
    "        \n",
    "    def get_average_metrics(self):\n",
    "        \"\"\"Calcula m√©tricas promedio\"\"\"\n",
    "        return {\n",
    "            'avg_gpu_memory': np.mean(self.metrics['gpu_memory_used']) if self.metrics['gpu_memory_used'] else 0,\n",
    "            'avg_gpu_utilization': np.mean(self.metrics['gpu_utilization']) if self.metrics['gpu_utilization'] else 0,\n",
    "            'avg_cpu_percent': np.mean(self.metrics['cpu_percent']) if self.metrics['cpu_percent'] else 0,\n",
    "            'avg_ram_used': np.mean(self.metrics['ram_used']) if self.metrics['ram_used'] else 0,\n",
    "            'total_time': self.metrics['timestamps'][-1] if self.metrics['timestamps'] else 0\n",
    "        }\n",
    "\n",
    "def print_system_info():\n",
    "    \"\"\"Muestra informaci√≥n del sistema\"\"\"\n",
    "    print(\"üñ•Ô∏è INFORMACI√ìN DEL SISTEMA\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üêç Python: {torch.__version__}\")\n",
    "    print(f\"üî• PyTorch: {torch.__version__}\")\n",
    "    print(f\"üöÄ CUDA disponible: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üìä GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"üéØ Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"üíæ RAM total: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "    print(f\"üîÑ CPUs: {psutil.cpu_count()}\")\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üìä Paso 4: Creaci√≥n de Dataset Sint√©tico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2851b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìä CREACI√ìN DE DATASET SFT CON GEMINI 2.0 FLASH\n",
    "# ============================================================\n",
    "\n",
    "# Instalar Google Generative AI para Gemini 2.0 Flash\n",
    "%pip install google-generativeai\n",
    "\n",
    "import google.generativeai as genai\n",
    "import random\n",
    "\n",
    "# ‚ö†Ô∏è CONFIGURAR API KEY DE GEMINI\n",
    "# Opci√≥n 1: Variable de entorno (recomendado)\n",
    "import os\n",
    "GEMINI_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# Opci√≥n 2: Directamente (NO recomendado para producci√≥n)\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"‚ö†Ô∏è  ATENCI√ìN: Configura tu API key de Google Gemini\")\n",
    "    print(\"üí° Obt√©n tu key en: https://aistudio.google.com/app/apikey\")\n",
    "    GEMINI_API_KEY = input(\"üîë Ingresa tu API key de Gemini: \").strip()\n",
    "\n",
    "# Configurar Gemini\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "def create_sft_dataset_with_gemini(num_samples=200):\n",
    "    \"\"\"\n",
    "    Crea un dataset sint√©tico de alta calidad usando Gemini 2.0 Flash\n",
    "    para Supervised Fine-Tuning (SFT) de alineaci√≥n de modelos.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ü§ñ Inicializando Gemini 2.0 Flash...\")\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
    "    \n",
    "    # Categor√≠as de datos para SFT de alineaci√≥n\n",
    "    sft_categories = [\n",
    "        {\n",
    "            \"category\": \"helpful_assistant\",\n",
    "            \"prompt\": \"\"\"Genera una conversaci√≥n entre un usuario y un asistente IA √∫til, honesto y preciso. \n",
    "            El asistente debe dar respuestas detalladas, bien estructuradas y con ejemplos pr√°cticos.\n",
    "            Formato: Usuario: [pregunta] | Asistente: [respuesta completa y √∫til]\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"safety_guidelines\", \n",
    "            \"prompt\": \"\"\"Genera una conversaci√≥n donde el usuario hace una pregunta que requiere consideraciones de seguridad o √©tica,\n",
    "            y el asistente responde de manera responsable, explicando los riesgos y alternativas seguras.\n",
    "            Formato: Usuario: [pregunta con implicaciones de seguridad] | Asistente: [respuesta responsable y segura]\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"educational_content\",\n",
    "            \"prompt\": \"\"\"Genera una conversaci√≥n educativa donde el usuario pregunta sobre conceptos t√©cnicos complejos\n",
    "            y el asistente explica de manera clara, progresiva y con analog√≠as apropiadas.\n",
    "            Formato: Usuario: [pregunta t√©cnica] | Asistente: [explicaci√≥n educativa clara]\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"problem_solving\",\n",
    "            \"prompt\": \"\"\"Genera una conversaci√≥n donde el usuario presenta un problema espec√≠fico\n",
    "            y el asistente ofrece un enfoque estructurado de resoluci√≥n paso a paso.\n",
    "            Formato: Usuario: [problema espec√≠fico] | Asistente: [soluci√≥n estructurada paso a paso]\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"refusal_harmful\",\n",
    "            \"prompt\": \"\"\"Genera una conversaci√≥n donde el usuario hace una solicitud inapropiada o da√±ina,\n",
    "            y el asistente rechaza educadamente pero ofrece alternativas constructivas.\n",
    "            Formato: Usuario: [solicitud inapropiada] | Asistente: [rechazo educado + alternativa constructiva]\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    dataset_entries = []\n",
    "    samples_per_category = max(1, num_samples // len(sft_categories))\n",
    "    \n",
    "    print(f\"üéØ Generando {num_samples} muestras de alta calidad...\")\n",
    "    print(f\"üìä {samples_per_category} muestras por categor√≠a\")\n",
    "    \n",
    "    for category_info in sft_categories:\n",
    "        category = category_info[\"category\"]\n",
    "        base_prompt = category_info[\"prompt\"]\n",
    "        \n",
    "        print(f\"\\nüîÑ Generando categor√≠a: {category}\")\n",
    "        \n",
    "        for i in range(samples_per_category):\n",
    "            try:\n",
    "                # Prompt espec√≠fico para cada muestra\n",
    "                full_prompt = f\"\"\"\n",
    "                {base_prompt}\n",
    "                \n",
    "                INSTRUCCIONES ESPEC√çFICAS:\n",
    "                - Genera UNA conversaci√≥n completa y realista\n",
    "                - El usuario debe hacer una pregunta natural y espec√≠fica\n",
    "                - El asistente debe dar una respuesta completa, √∫til y bien estructurada\n",
    "                - Usa un tono profesional pero accesible\n",
    "                - La respuesta debe tener entre 100-300 palabras\n",
    "                - Incluye ejemplos pr√°cticos cuando sea apropiado\n",
    "                \n",
    "                IMPORTANTE: Responde SOLO con el formato solicitado, sin explicaciones adicionales.\n",
    "                \"\"\"\n",
    "                \n",
    "                # Generar contenido con Gemini\n",
    "                response = model.generate_content(full_prompt)\n",
    "                generated_text = response.text.strip()\n",
    "                \n",
    "                # Procesar la respuesta de Gemini\n",
    "                if '|' in generated_text:\n",
    "                    parts = generated_text.split('|', 1)\n",
    "                    user_part = parts[0].replace('Usuario:', '').strip()\n",
    "                    assistant_part = parts[1].replace('Asistente:', '').strip()\n",
    "                    \n",
    "                    # Formato est√°ndar para SFT\n",
    "                    conversation = f\"<|user|>\\n{user_part}\\n<|assistant|>\\n{assistant_part}\"\n",
    "                    \n",
    "                    dataset_entries.append({\n",
    "                        \"text\": conversation,\n",
    "                        \"category\": category,\n",
    "                        \"length\": len(conversation),\n",
    "                        \"tokens_estimate\": len(conversation.split())\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"  ‚úÖ Muestra {i+1}/{samples_per_category} generada\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è  Formato incorrecto en muestra {i+1}, reintentando...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error generando muestra {i+1}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Crear dataset de Hugging Face\n",
    "    print(f\"\\nüì¶ Creando dataset con {len(dataset_entries)} muestras v√°lidas...\")\n",
    "    dataset = Dataset.from_list(dataset_entries)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def show_dataset_stats(dataset):\n",
    "    \"\"\"Muestra estad√≠sticas detalladas del dataset\"\"\"\n",
    "    print(\"\\nüìà ESTAD√çSTICAS DEL DATASET SFT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Estad√≠sticas b√°sicas\n",
    "    print(f\"üìä Total de muestras: {len(dataset)}\")\n",
    "    print(f\"üìè Longitud promedio: {np.mean([item['length'] for item in dataset]):.1f} caracteres\")\n",
    "    print(f\"üéØ Tokens estimados promedio: {np.mean([item['tokens_estimate'] for item in dataset]):.1f}\")\n",
    "    \n",
    "    # Distribuci√≥n por categor√≠as\n",
    "    if 'category' in dataset.column_names:\n",
    "        categories = {}\n",
    "        for item in dataset:\n",
    "            cat = item['category']\n",
    "            categories[cat] = categories.get(cat, 0) + 1\n",
    "        \n",
    "        print(\"\\nüè∑Ô∏è  DISTRIBUCI√ìN POR CATEGOR√çAS:\")\n",
    "        for cat, count in categories.items():\n",
    "            print(f\"  ‚Ä¢ {cat}: {count} muestras\")\n",
    "    \n",
    "    print(\"\\nüìù EJEMPLO DE CONVERSACI√ìN GENERADA:\")\n",
    "    print(\"-\" * 60)\n",
    "    example = dataset[0]['text']\n",
    "    print(example[:400] + \"...\" if len(example) > 400 else example)\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# üöÄ GENERAR DATASET CON GEMINI 2.0 FLASH\n",
    "print(\"üöÄ CREANDO DATASET SFT CON GEMINI 2.0 FLASH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    dataset = create_sft_dataset_with_gemini(num_samples=100)  # Empezamos con 100 para pruebas\n",
    "    show_dataset_stats(dataset)\n",
    "    \n",
    "    print(\"\\n‚úÖ ¬°Dataset SFT de alta calidad creado exitosamente!\")\n",
    "    print(\"üéØ Perfecto para entrenar modelos alineados y √∫tiles\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creando dataset: {str(e)}\")\n",
    "    print(\"\\nüîÑ Creando dataset de respaldo b√°sico...\")\n",
    "    \n",
    "    # Dataset de respaldo si Gemini falla\n",
    "    fallback_data = [\n",
    "        {\n",
    "            \"text\": \"<|user|>\\n¬øC√≥mo puedo implementar una funci√≥n recursiva en Python?\\n<|assistant|>\\nUna funci√≥n recursiva es aquella que se llama a s√≠ misma. Aqu√≠ te explico c√≥mo implementarla:\\n\\n1. **Caso base**: Condici√≥n que detiene la recursi√≥n\\n2. **Caso recursivo**: La funci√≥n se llama a s√≠ misma\\n\\nEjemplo - Factorial:\\n```python\\ndef factorial(n):\\n    if n <= 1:  # Caso base\\n        return 1\\n    return n * factorial(n-1)  # Caso recursivo\\n```\\n\\n**Consejos importantes**:\\n- Siempre define un caso base claro\\n- Aseg√∫rate de que la recursi√≥n progrese hacia el caso base\\n- Considera el l√≠mite de recursi√≥n de Python (1000 por defecto)\", \n",
    "            \"category\": \"educational\", \n",
    "            \"length\": 500, \n",
    "            \"tokens_estimate\": 120\n",
    "        }\n",
    "    ]\n",
    "    dataset = Dataset.from_list(fallback_data)\n",
    "    show_dataset_stats(dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ü§ñ Paso 5: Configuraci√≥n del Modelo Base"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cea626d9",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "def setup_huggingface_auth():\n",
    "    \"\"\"Configura la autenticaci√≥n con Hugging Face\"\"\"\n",
    "    print(\"üîë CONFIGURACI√ìN DE HUGGING FACE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Opci√≥n 1: Variable de entorno (recomendado)\n",
    "    hf_token = os.getenv('HF_TOKEN') or os.getenv('HUGGINGFACE_HUB_TOKEN')\n",
    "    \n",
    "    if hf_token:\n",
    "        print(\"‚úÖ Token encontrado en variables de entorno\")\n",
    "        login(token=hf_token)\n",
    "        print(\"üöÄ Autenticado correctamente con Hugging Face\")\n",
    "        return True\n",
    "    \n",
    "    # Opci√≥n 2: Login interactivo\n",
    "    print(\"‚ö†Ô∏è  No se encontr√≥ token en variables de entorno\")\n",
    "    print(\"üí° Obt√©n tu token en: https://huggingface.co/settings/tokens\")\n",
    "    print(\"üîß Opciones:\")\n",
    "    print(\"  1. Ingresar token manualmente\")\n",
    "    print(\"  2. Usar modelo p√∫blico (puede tener limitaciones)\")\n",
    "    \n",
    "    choice = input(\"Selecciona una opci√≥n (1/2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        hf_token = input(\"üîë Ingresa tu token de Hugging Face: \").strip()\n",
    "        if hf_token:\n",
    "            try:\n",
    "                login(token=hf_token)\n",
    "                print(\"‚úÖ Autenticado correctamente!\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error de autenticaci√≥n: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"‚ùå Token vac√≠o\")\n",
    "            return False\n",
    "    \n",
    "    elif choice == \"2\":\n",
    "        print(\"‚ö†Ô∏è  Continuando sin autenticaci√≥n\")\n",
    "        print(\"üìù Nota: Solo podr√°s acceder a modelos p√∫blicos\")\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå Opci√≥n no v√°lida\")\n",
    "        return False\n",
    "\n",
    "# Configurar autenticaci√≥n\n",
    "if not setup_huggingface_auth():\n",
    "    print(\"‚ùå No se pudo configurar la autenticaci√≥n\")\n",
    "    print(\"üîÑ Puedes intentar configurar manualmente:\")\n",
    "    print(\"   export HF_TOKEN='tu_token_aqui'\")\n",
    "    print(\"   o usar: huggingface-cli login\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n del sistema\n",
    "print_system_info()\n",
    "\n",
    "# Configuraci√≥n del modelo\n",
    "MODEL_NAME = \"microsoft/DialoGPT-small\"  # Modelo peque√±o para demostraci√≥n\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "print(f\"\\\\nü§ñ Configuraci√≥n del modelo:\")\n",
    "print(f\"üì¶ Modelo: {MODEL_NAME}\")\n",
    "print(f\"üìè Longitud m√°xima: {MAX_LENGTH}\")\n",
    "print(f\"üî¢ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"üîÑ √âpocas: {NUM_EPOCHS}\")\n",
    "print(f\"üìà Learning rate: {LEARNING_RATE}\")\n",
    "\n",
    "# Cargar tokenizer\n",
    "print(\"\\\\nüî§ Cargando tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"‚úÖ Tokenizer cargado correctamente\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üîÑ Paso 6: Preparaci√≥n de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f03e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocesa los datos para el entrenamiento\"\"\"\n",
    "    # Tokenizar los textos\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Para language modeling, labels = input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Preprocesar dataset\n",
    "print(\"üîÑ Preprocesando dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Dividir en train/test\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"‚úÖ Datos preparados:\")\n",
    "print(f\"üéØ Training samples: {len(train_dataset)}\")\n",
    "print(f\"üìä Evaluation samples: {len(eval_dataset)}\")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # No masked language modeling para GPT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e842c49",
   "metadata": {},
   "source": [
    "## ‚ö° Paso 8: Configuraci√≥n de DeepSpeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear configuraci√≥n de DeepSpeed\n",
    "deepspeed_config = {\n",
    "    \"train_batch_size\": BATCH_SIZE,\n",
    "    \"train_micro_batch_size_per_gpu\": BATCH_SIZE,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": LEARNING_RATE,\n",
    "            \"betas\": [0.9, 0.999],\n",
    "            \"eps\": 1e-8,\n",
    "            \"weight_decay\": 0.01\n",
    "        }\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": 0,\n",
    "            \"warmup_max_lr\": LEARNING_RATE,\n",
    "            \"warmup_num_steps\": 50\n",
    "        }\n",
    "    },\n",
    "    \"fp16\": {\n",
    "        \"enabled\": True,\n",
    "        \"loss_scale\": 0,\n",
    "        \"initial_scale_power\": 16,\n",
    "        \"loss_scale_window\": 1000,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"allgather_partitions\": True,\n",
    "        \"allgather_bucket_size\": 2e8,\n",
    "        \"reduce_scatter\": True,\n",
    "        \"reduce_bucket_size\": 2e8,\n",
    "        \"overlap_comm\": True,\n",
    "        \"contiguous_gradients\": True\n",
    "    },\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"wall_clock_breakdown\": False\n",
    "}\n",
    "\n",
    "# Guardar configuraci√≥n\n",
    "with open(\"deepspeed_config.json\", \"w\") as f:\n",
    "    json.dump(deepspeed_config, f, indent=2)\n",
    "\n",
    "print(\"‚ö° CONFIGURACI√ìN DEEPSPEED CREADA\")\n",
    "print(\"=\" * 40)\n",
    "print(\"üéØ Optimizaciones habilitadas:\")\n",
    "print(\"  ‚úÖ FP16 (Half Precision)\")\n",
    "print(\"  ‚úÖ ZeRO Stage 2 (Optimizer State Partitioning)\")\n",
    "print(\"  ‚úÖ Gradient Clipping\")\n",
    "print(\"  ‚úÖ Comunicaci√≥n Optimizada\")\n",
    "print(\"  ‚úÖ AdamW Optimizer\")\n",
    "print(\"  ‚úÖ Warmup Learning Rate Scheduler\")\n",
    "print(\"=\" * 40)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üêå Paso 7: Entrenamiento  DeepSpeed (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9320eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "def train_baseline_model():\n",
    "    \"\"\"Entrena el modelo SIN DeepSpeed usando SFTTrainer + LoRA\"\"\"\n",
    "    \n",
    "    print(\"üêå INICIANDO ENTRENAMIENTO BASELINE (SIN DEEPSPEED)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Limpiar memoria GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Configuraci√≥n LoRA para PEFT\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,                           # Rango de LoRA\n",
    "        lora_alpha=32,                  # Par√°metro de escalado\n",
    "        lora_dropout=0.05,              # Dropout para regularizaci√≥n\n",
    "        target_modules=\"all-linear\",    # Aplicar a todas las capas lineales\n",
    "        modules_to_save=[\"lm_head\", \"embed_tokens\"],  # M√≥dulos a guardar completos\n",
    "        task_type=TaskType.CAUSAL_LM,   # Tipo de tarea\n",
    "        bias=\"none\"                     # No entrenar bias\n",
    "    )\n",
    "    \n",
    "    # Configuraci√≥n de entrenamiento SFT\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=\"./baseline_sft_results\",\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        warmup_steps=50,\n",
    "        logging_steps=20,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=None,\n",
    "        # Configuraciones espec√≠ficas de SFT\n",
    "        max_seq_length=MAX_LENGTH,\n",
    "        packing=False,                  # No empaquetar secuencias para simplicidad\n",
    "        dataset_text_field=\"text\",      # Campo que contiene el texto\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    print(\"ü§ñ Cargando modelo base...\")\n",
    "    print(f\"üì¶ Modelo: {MODEL_NAME}\")\n",
    "    \n",
    "    # Inicializar monitor de rendimiento\n",
    "    monitor = PerformanceMonitor()\n",
    "    monitor.start_monitoring()\n",
    "    \n",
    "    # Crear SFTTrainer (carga el modelo autom√°ticamente)\n",
    "    trainer = SFTTrainer(\n",
    "        model=MODEL_NAME,              # Cargar directamente por nombre\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        args=sft_config,\n",
    "        peft_config=peft_config,       # Configuraci√≥n LoRA\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=None,            # SFTTrainer maneja esto autom√°ticamente\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ SFTTrainer configurado con LoRA\")\n",
    "    print(f\"üéØ Par√°metros entrenables: {trainer.model.print_trainable_parameters()}\")\n",
    "    \n",
    "    # Entrenar\n",
    "    print(\"üöÄ Iniciando entrenamiento SFT...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Callback para monitoreo durante entrenamiento\n",
    "    class MonitoringCallback:\n",
    "        def __init__(self, monitor):\n",
    "            self.monitor = monitor\n",
    "            \n",
    "        def on_step_end(self, args, state, control, logs=None, **kwargs):\n",
    "            self.monitor.log_metrics()\n",
    "    \n",
    "    trainer.add_callback(MonitoringCallback(monitor))\n",
    "    \n",
    "    # ¬°Entrenar!\n",
    "    trainer.train()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    # Obtener m√©tricas finales\n",
    "    final_metrics = monitor.get_average_metrics()\n",
    "    final_metrics['total_training_time'] = training_time\n",
    "    \n",
    "    print(f\"\\n‚úÖ Entrenamiento baseline completado en {training_time:.2f} segundos\")\n",
    "    \n",
    "    # Guardar el modelo entrenado\n",
    "    trainer.save_model(\"./baseline_sft_final\")\n",
    "    print(\"üíæ Modelo baseline guardado en ./baseline_sft_final\")\n",
    "    \n",
    "    # Limpiar memoria\n",
    "    del trainer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return final_metrics, monitor.metrics\n",
    "\n",
    "# ============================================================\n",
    "# üöÄ PASO 9: ENTRENAMIENTO CON DEEPSPEED \n",
    "# ============================================================\n",
    "\n",
    "def train_deepspeed_model():\n",
    "    \"\"\"Entrena el modelo CON DeepSpeed usando SFTTrainer + LoRA\"\"\"\n",
    "    \n",
    "    print(\"üöÄ INICIANDO ENTRENAMIENTO CON DEEPSPEED\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Limpiar memoria GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Misma configuraci√≥n LoRA (para comparaci√≥n justa)\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=\"all-linear\",\n",
    "        modules_to_save=[\"lm_head\", \"embed_tokens\"],\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    \n",
    "    # Configuraci√≥n SFT con DeepSpeed\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=\"./deepspeed_sft_results\",\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        warmup_steps=50,\n",
    "        logging_steps=20,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=None,\n",
    "        # Configuraciones espec√≠ficas de SFT\n",
    "        max_seq_length=MAX_LENGTH,\n",
    "        packing=False,\n",
    "        dataset_text_field=\"text\",\n",
    "        seed=42,\n",
    "        # ‚ö° CONFIGURACI√ìN DEEPSPEED\n",
    "        deepspeed=\"deepspeed_config.json\",  # ¬°Aqu√≠ est√° la magia!\n",
    "        fp16=True,                          # Half precision\n",
    "        dataloader_num_workers=0,           # Para evitar problemas con DeepSpeed\n",
    "    )\n",
    "    \n",
    "    print(\"ü§ñ Cargando modelo base con optimizaciones DeepSpeed...\")\n",
    "    print(f\"üì¶ Modelo: {MODEL_NAME}\")\n",
    "    print(\"‚ö° Optimizaciones: ZeRO Stage 2 + FP16 + LoRA\")\n",
    "    \n",
    "    # Inicializar monitor de rendimiento\n",
    "    monitor = PerformanceMonitor()\n",
    "    monitor.start_monitoring()\n",
    "    \n",
    "    # Crear SFTTrainer con DeepSpeed\n",
    "    trainer = SFTTrainer(\n",
    "        model=MODEL_NAME,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        args=sft_config,\n",
    "        peft_config=peft_config,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=None,\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ SFTTrainer configurado con DeepSpeed + LoRA\")\n",
    "    print(f\"üéØ Par√°metros entrenables: {trainer.model.print_trainable_parameters()}\")\n",
    "    \n",
    "    # Entrenar\n",
    "    print(\"üöÄ Iniciando entrenamiento SFT con DeepSpeed...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Callback para monitoreo\n",
    "    class MonitoringCallback:\n",
    "        def __init__(self, monitor):\n",
    "            self.monitor = monitor\n",
    "            \n",
    "        def on_step_end(self, args, state, control, logs=None, **kwargs):\n",
    "            self.monitor.log_metrics()\n",
    "    \n",
    "    trainer.add_callback(MonitoringCallback(monitor))\n",
    "    \n",
    "    # ¬°Entrenar con DeepSpeed!\n",
    "    trainer.train()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    # Obtener m√©tricas finales\n",
    "    final_metrics = monitor.get_average_metrics()\n",
    "    final_metrics['total_training_time'] = training_time\n",
    "    \n",
    "    print(f\"\\n‚úÖ Entrenamiento con DeepSpeed completado en {training_time:.2f} segundos\")\n",
    "    \n",
    "    # Guardar el modelo entrenado\n",
    "    trainer.save_model(\"./deepspeed_sft_final\")\n",
    "    print(\"üíæ Modelo DeepSpeed guardado en ./deepspeed_sft_final\")\n",
    "    \n",
    "    # Limpiar memoria\n",
    "    del trainer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return final_metrics, monitor.metrics\n",
    "\n",
    "# ============================================================\n",
    "# üîÑ EJECUTAR ENTRENAMIENTOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"üéØ CONFIGURACI√ìN DEL EXPERIMENTO:\")\n",
    "print(f\"üì¶ Modelo base: {MODEL_NAME}\")\n",
    "print(f\"üìä Dataset size: {len(train_dataset)} samples\")\n",
    "print(f\"üî¢ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"üîÑ √âpocas: {NUM_EPOCHS}\")\n",
    "print(f\"üìè Max length: {MAX_LENGTH}\")\n",
    "print(f\"üéØ LoRA rank: 16\")\n",
    "\n",
    "# Ejecutar entrenamiento baseline\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "baseline_metrics, baseline_detailed = train_baseline_model()\n",
    "\n",
    "print(\"\\nüìä M√âTRICAS BASELINE:\")\n",
    "print(f\"‚è±Ô∏è  Tiempo total: {baseline_metrics['total_training_time']:.2f}s\")\n",
    "print(f\"üéØ GPU memoria promedio: {baseline_metrics['avg_gpu_memory']:.1f} MB\")\n",
    "print(f\"üìà GPU utilizaci√≥n promedio: {baseline_metrics['avg_gpu_utilization']:.1f}%\")\n",
    "print(f\"üîÑ CPU promedio: {baseline_metrics['avg_cpu_percent']:.1f}%\")\n",
    "print(f\"üíæ RAM promedio: {baseline_metrics['avg_ram_used']:.1f} GB\")\n",
    "\n",
    "# Ejecutar entrenamiento con DeepSpeed\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "deepspeed_metrics, deepspeed_detailed = train_deepspeed_model()\n",
    "\n",
    "print(\"\\nüìä M√âTRICAS DEEPSPEED:\")\n",
    "print(f\"‚è±Ô∏è  Tiempo total: {deepspeed_metrics['total_training_time']:.2f}s\")\n",
    "print(f\"üéØ GPU memoria promedio: {deepspeed_metrics['avg_gpu_memory']:.1f} MB\")\n",
    "print(f\"üìà GPU utilizaci√≥n promedio: {deepspeed_metrics['avg_gpu_utilization']:.1f}%\")\n",
    "print(f\"üîÑ CPU promedio: {deepspeed_metrics['avg_cpu_percent']:.1f}%\")\n",
    "print(f\"üíæ RAM promedio: {deepspeed_metrics['avg_ram_used']:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üìà Paso 10: An√°lisis y Comparaci√≥n de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3107433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_improvements(baseline, deepspeed):\n",
    "    \"\"\"Calcula las mejoras de DeepSpeed vs baseline\"\"\"\n",
    "    improvements = {}\n",
    "    \n",
    "    # Tiempo de entrenamiento (menor es mejor)\n",
    "    time_improvement = ((baseline['total_training_time'] - deepspeed['total_training_time']) / \n",
    "                       baseline['total_training_time']) * 100\n",
    "    improvements['time_speedup'] = time_improvement\n",
    "    \n",
    "    # Memoria GPU (menor es mejor)\n",
    "    memory_improvement = ((baseline['avg_gpu_memory'] - deepspeed['avg_gpu_memory']) / \n",
    "                         baseline['avg_gpu_memory']) * 100\n",
    "    improvements['memory_reduction'] = memory_improvement\n",
    "    \n",
    "    # Utilizaci√≥n GPU (mayor es mejor, pero calculamos eficiencia)\n",
    "    gpu_efficiency = (deepspeed['avg_gpu_utilization'] / baseline['avg_gpu_utilization'] - 1) * 100\n",
    "    improvements['gpu_efficiency'] = gpu_efficiency\n",
    "    \n",
    "    return improvements\n",
    "\n",
    "# Calcular mejoras\n",
    "improvements = calculate_improvements(baseline_metrics, deepspeed_metrics)\n",
    "\n",
    "# Crear tabla de comparaci√≥n\n",
    "comparison_data = {\n",
    "    'M√©trica': [\n",
    "        'Tiempo de Entrenamiento (s)',\n",
    "        'Memoria GPU Promedio (MB)',\n",
    "        'Utilizaci√≥n GPU Promedio (%)',\n",
    "        'CPU Promedio (%)',\n",
    "        'RAM Promedio (GB)'\n",
    "    ],\n",
    "    'Baseline (Sin DeepSpeed)': [\n",
    "        f\"{baseline_metrics['total_training_time']:.2f}\",\n",
    "        f\"{baseline_metrics['avg_gpu_memory']:.1f}\",\n",
    "        f\"{baseline_metrics['avg_gpu_utilization']:.1f}\",\n",
    "        f\"{baseline_metrics['avg_cpu_percent']:.1f}\",\n",
    "        f\"{baseline_metrics['avg_ram_used']:.1f}\"\n",
    "    ],\n",
    "    'DeepSpeed Optimizado': [\n",
    "        f\"{deepspeed_metrics['total_training_time']:.2f}\",\n",
    "        f\"{deepspeed_metrics['avg_gpu_memory']:.1f}\",\n",
    "        f\"{deepspeed_metrics['avg_gpu_utilization']:.1f}\",\n",
    "        f\"{deepspeed_metrics['avg_cpu_percent']:.1f}\",\n",
    "        f\"{deepspeed_metrics['avg_ram_used']:.1f}\"\n",
    "    ],\n",
    "    'Mejora (%)': [\n",
    "        f\"{improvements['time_speedup']:+.1f}\",\n",
    "        f\"{improvements['memory_reduction']:+.1f}\",\n",
    "        f\"{improvements['gpu_efficiency']:+.1f}\",\n",
    "        \"N/A\",\n",
    "        \"N/A\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"üèÜ COMPARACI√ìN DE RESULTADOS\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Resumen de beneficios\n",
    "print(\"\\\\nüéØ RESUMEN DE BENEFICIOS DE DEEPSPEED:\")\n",
    "print(f\"‚ö° Aceleraci√≥n del entrenamiento: {improvements['time_speedup']:+.1f}%\")\n",
    "print(f\"üíæ Reducci√≥n de memoria GPU: {improvements['memory_reduction']:+.1f}%\")\n",
    "print(f\"üìà Mejora en eficiencia GPU: {improvements['gpu_efficiency']:+.1f}%\")\n",
    "\n",
    "if improvements['time_speedup'] > 0:\n",
    "    print(f\"\\\\nüöÄ El entrenamiento fue {improvements['time_speedup']:.1f}% m√°s r√°pido con DeepSpeed!\")\n",
    "if improvements['memory_reduction'] > 0:\n",
    "    print(f\"üí° Se redujo el uso de memoria GPU en {improvements['memory_reduction']:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üìä Paso 11: Visualizaci√≥n de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec6158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear visualizaciones\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('üöÄ Comparaci√≥n de Rendimiento: Baseline vs DeepSpeed', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Tiempo de entrenamiento\n",
    "ax1 = axes[0, 0]\n",
    "methods = ['Baseline\\\\n(Sin DeepSpeed)', 'DeepSpeed\\\\n(Optimizado)']\n",
    "times = [baseline_metrics['total_training_time'], deepspeed_metrics['total_training_time']]\n",
    "colors = ['#ff7f7f', '#7fbf7f']\n",
    "\n",
    "bars1 = ax1.bar(methods, times, color=colors, alpha=0.8, edgecolor='black', linewidth=1)\n",
    "ax1.set_title('‚è±Ô∏è Tiempo de Entrenamiento', fontweight='bold')\n",
    "ax1.set_ylabel('Tiempo (segundos)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Agregar valores en las barras\n",
    "for bar, time in zip(bars1, times):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{time:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Memoria GPU\n",
    "ax2 = axes[0, 1]\n",
    "memory_usage = [baseline_metrics['avg_gpu_memory'], deepspeed_metrics['avg_gpu_memory']]\n",
    "\n",
    "bars2 = ax2.bar(methods, memory_usage, color=colors, alpha=0.8, edgecolor='black', linewidth=1)\n",
    "ax2.set_title('üéØ Uso de Memoria GPU', fontweight='bold')\n",
    "ax2.set_ylabel('Memoria (MB)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, memory in zip(bars2, memory_usage):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{memory:.0f}MB', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Utilizaci√≥n GPU\n",
    "ax3 = axes[1, 0]\n",
    "gpu_util = [baseline_metrics['avg_gpu_utilization'], deepspeed_metrics['avg_gpu_utilization']]\n",
    "\n",
    "bars3 = ax3.bar(methods, gpu_util, color=colors, alpha=0.8, edgecolor='black', linewidth=1)\n",
    "ax3.set_title('üìà Utilizaci√≥n GPU', fontweight='bold')\n",
    "ax3.set_ylabel('Utilizaci√≥n (%)')\n",
    "ax3.set_ylim(0, 100)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, util in zip(bars3, gpu_util):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "             f'{util:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Gr√°fico de mejoras\n",
    "ax4 = axes[1, 1]\n",
    "improvement_metrics = ['Velocidad\\\\nEntrenamiento', 'Reducci√≥n\\\\nMemoria GPU']\n",
    "improvement_values = [improvements['time_speedup'], improvements['memory_reduction']]\n",
    "improvement_colors = ['green' if x > 0 else 'red' for x in improvement_values]\n",
    "\n",
    "bars4 = ax4.bar(improvement_metrics, improvement_values, color=improvement_colors, alpha=0.7, edgecolor='black', linewidth=1)\n",
    "ax4.set_title('üèÜ Mejoras con DeepSpeed', fontweight='bold')\n",
    "ax4.set_ylabel('Mejora (%)')\n",
    "ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, improvement in zip(bars4, improvement_values):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + (1 if height > 0 else -3),\n",
    "             f'{improvement:+.1f}%', ha='center', va='bottom' if height > 0 else 'top', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gr√°fico de l√≠nea temporal para memoria GPU\n",
    "if len(baseline_detailed['timestamps']) > 0 and len(deepspeed_detailed['timestamps']) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(baseline_detailed['timestamps'], baseline_detailed['gpu_memory_used'], \n",
    "             label='Baseline (Sin DeepSpeed)', color='red', linewidth=2, alpha=0.8)\n",
    "    plt.plot(deepspeed_detailed['timestamps'], deepspeed_detailed['gpu_memory_used'], \n",
    "             label='DeepSpeed (Optimizado)', color='green', linewidth=2, alpha=0.8)\n",
    "    \n",
    "    plt.title('üìà Uso de Memoria GPU Durante el Entrenamiento', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Tiempo (segundos)')\n",
    "    plt.ylabel('Memoria GPU (MB)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
